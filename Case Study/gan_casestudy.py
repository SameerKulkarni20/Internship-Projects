# -*- coding: utf-8 -*-
"""GAN_casestudy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12EALY5mx-uyRschiDeMvFOR7uZ54eoft
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Reshape, Flatten, LeakyReLU
from tensorflow.keras.models import Sequential

# Load and preprocess the dataset
with np.load('/content/mnist.npz') as f:
    x_train = f['x_train'].reshape(-1, 784) / 255.0

# Generator model
generator = Sequential([
    Dense(128, activation=LeakyReLU(0.2), input_dim=100),
    Dense(784, activation='sigmoid'),
    Reshape((28, 28))
])

# Discriminator modela
discriminator = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the discriminator
discriminator.compile(loss='binary_crossentropy', optimizer='adam')

# Combined GAN model
discriminator.trainable = False
gan = Sequential([generator, discriminator])
gan.compile(loss='binary_crossentropy', optimizer='adam')

# Training loop
for epoch in range(10):  # Reduced epochs for simplicity
    noise = tf.random.normal([100, 100])
    generated_images = generator.predict(noise)
    real_images = x_train[:100].reshape(100, 28, 28)

    # Train discriminator
    x = np.concatenate([real_images, generated_images])
    y = np.concatenate([np.ones((100, 1)), np.zeros((100, 1))])
    d_loss = discriminator.train_on_batch(x, y)

    # Train generator
    y_gen = np.ones((100, 1))
    g_loss = gan.train_on_batch(noise, y_gen)

    # Print losses; use indexing if they are lists
    d_loss = d_loss[0] if isinstance(d_loss, list) else d_loss
    g_loss = g_loss[0] if isinstance(g_loss, list) else g_loss
    print(f"Epoch {epoch+1}, D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}")

