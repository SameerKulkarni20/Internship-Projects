# -*- coding: utf-8 -*-
"""RNN_casestudy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W9Ts2P8xBxIVUJGLOPbRREus6DZ2BiGo
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load your text data
text = """Cognifront mentors helping students with their project work.
Show a group of 3-4 students with 1-2 mentors. Focus on laptop, code, IoT kit and personal interaction.
The team works together on collaborative projects, ensuring hands-on learning."""

# Tokenize words instead of characters
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
word_to_int = tokenizer.word_index
int_to_word = {v: k for k, v in word_to_int.items()}

# Convert text to sequences of words
seq_length = 5  # Length of word sequences (feel free to adjust this)
encoded = tokenizer.texts_to_sequences([text])[0]

# Prepare the dataset
data = []
for i in range(len(encoded) - seq_length):
    seq_in = encoded[i:i + seq_length]
    seq_out = encoded[i + seq_length]
    data.append((seq_in, seq_out))

# Extract x_train and y_train
x_train = np.array([x[0] for x in data])
y_train = np.array([x[1] for x in data])

# Create the RNN model
vocab_size = len(word_to_int) + 1  # Add 1 for padding
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, 10, input_length=seq_length),  # Adjust embedding size as needed
    tf.keras.layers.SimpleRNN(128, return_sequences=True),
    tf.keras.layers.SimpleRNN(128),
    tf.keras.layers.Dense(vocab_size, activation='softmax')
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=100, batch_size=2)  # Increase epochs for better learning

# Evaluate the model's accuracy
accuracy = model.evaluate(x_train, y_train)
print(f"Training accuracy: {accuracy[1] * 100:.2f}%")

# Generate new text starting from a sequence
start = "Show a group of"  # Use a valid starting sequence from the training data
encoded_start = tokenizer.texts_to_sequences([start])[0]
generated_text = start

for i in range(30):  # Generate 30 words
    # Pad the sequence to match the input length
    pattern = pad_sequences([encoded_start[-seq_length:]], maxlen=seq_length, padding='pre')
    prediction = model.predict(pattern, verbose=0)
    index = np.argmax(prediction)
    result = int_to_word.get(index, "")
    generated_text += " " + result
    encoded_start.append(index)

print(generated_text)